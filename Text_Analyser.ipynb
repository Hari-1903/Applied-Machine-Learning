{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkHj0PAKm6e5iFPTpwk8TB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hari-1903/Applied-Machine-Learning/blob/main/Text_Analyser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analysing Text Data**"
      ],
      "metadata": {
        "id": "XQ6-uGjkgAZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing data using **tokenization**\n",
        "\n",
        "Tokenization is the process of dividing text into a set of meaningful pieces. These pieces are called\n",
        "tokens."
      ],
      "metadata": {
        "id": "8iOr9OOCgedI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsTBt2F0akWU",
        "outputId": "9128a175-289a-4d71-970c-398de9e48a50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Word Tokenizer:\n",
            "\n",
            "['Are you curious about tokenization?', \"Let's see how it works!\", 'We need to analyze a couple of sentences with punctuations to see it in action.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "text=\"Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action.\"\n",
        "sent_tokenize_list=sent_tokenize(text)\n",
        "print(\"Sentence Word Tokenizer:\\n\")\n",
        "print(sent_tokenize_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(\"Word Tokenizer: \\n\")\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB5Zabctcawd",
        "outputId": "0e483707-de04-4fea-da31-e4235254303e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenizer: \n",
            "\n",
            "['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', \"'s\", 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "word_punct_tokenizer = WordPunctTokenizer()\n",
        "print(\"\\nWord punct tokenizer:\")\n",
        "print(word_punct_tokenizer.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRQs3y9ucbMx",
        "outputId": "7ebc13bd-b8b7-43f2-f24d-dfb1e4d385ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word punct tokenizer:\n",
            "['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', \"'\", 's', 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stemming** text data\n",
        "\n",
        "The goal of stemming is to reduce these different forms into a common base form. This uses a heuristic process to\n",
        "cut off the ends of words to extract the base form."
      ],
      "metadata": {
        "id": "N5gQLTexgwIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "words=['airplane','eventually','dogs','eating','was','wolf','beaches','grounded','enjoying','goal','vision']\n",
        "stemmers=['PORTER','LANCASTER','SNOWBALL']\n",
        "\n",
        "stemmer_porter=PorterStemmer()\n",
        "stemmer_lancaster=LancasterStemmer()\n",
        "stemmer_snowball=SnowballStemmer('english')\n",
        "\n",
        "formatted_row='{:>16}'*(len(stemmers)+1)\n",
        "print('\\n',formatted_row.format('WORD',*stemmers),'\\n')\n",
        "\n",
        "for word in words:\n",
        "  stemmed_words=[stemmer_porter.stem(word),stemmer_lancaster.stem(word),stemmer_snowball.stem(word)]\n",
        "  print (formatted_row.format(word,*stemmed_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CrWZllndgU-",
        "outputId": "7fe37be9-8b87-421d-805d-a9cfe500a570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "             WORD          PORTER       LANCASTER        SNOWBALL \n",
            "\n",
            "        airplane         airplan           airpl         airplan\n",
            "      eventually          eventu              ev          eventu\n",
            "            dogs             dog             dog             dog\n",
            "          eating             eat             eat             eat\n",
            "             was              wa             was             was\n",
            "            wolf            wolf            wolf            wolf\n",
            "         beaches           beach           beach           beach\n",
            "        grounded          ground          ground          ground\n",
            "        enjoying           enjoy           enjoy           enjoy\n",
            "            goal            goal            goal            goal\n",
            "          vision          vision             vis          vision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting text to its base using **Lemmatization**\n",
        "\n",
        "The goal of lemmatization is also to reduce words to their base forms, but this is a more structured\n",
        "approach."
      ],
      "metadata": {
        "id": "4xu8Ia5ChHAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "words=['airplane','eventually','dogs','eating','was','wolf','beaches','grounded','enjoying','goal','vision']\n",
        "lemmatizers=['NOUN LEMMATIZER','VERB LEMATIZER']\n",
        "lemmatizer_wordnet=WordNetLemmatizer()\n",
        "\n",
        "formatted_row='{:>24}'*(len(lemmatizers) + 1)\n",
        "print('\\n',formatted_row.format('WORD',*lemmatizers),'\\n')\n",
        "\n",
        "for word in words:\n",
        "  lemmatized_words=[lemmatizer_wordnet.lemmatize(word,pos='n'),lemmatizer_wordnet.lemmatize(word,pos='v')]\n",
        "  print(formatted_row.format(word,*lemmatized_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6KHubnJhwuh",
        "outputId": "85b2f6d7-620f-45d6-d7a0-bc24e8ebaf0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                     WORD         NOUN LEMMATIZER          VERB LEMATIZER \n",
            "\n",
            "                airplane                airplane                airplane\n",
            "              eventually              eventually              eventually\n",
            "                    dogs                     dog                     dog\n",
            "                  eating                  eating                     eat\n",
            "                     was                      wa                      be\n",
            "                    wolf                    wolf                    wolf\n",
            "                 beaches                   beach                   beach\n",
            "                grounded                grounded                  ground\n",
            "                enjoying                enjoying                   enjoy\n",
            "                    goal                    goal                    goal\n",
            "                  vision                  vision                  vision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building **bag of words** model\n",
        "\n",
        "Algorithms need numerical data so that they can analyze them and output meaningful information.This is basically a model that\n",
        "learns a vocabulary from all the words in all the documents.\n",
        "\n"
      ],
      "metadata": {
        "id": "QEHKKwrffmO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chunking\n",
        "nltk.download('maxent_ne_chunker')\n",
        "\n",
        "import numpy as np\n",
        "from nltk.corpus import brown\n",
        "\n",
        "\n",
        "def splitter(data, num_words):\n",
        "  words = data.split(' ')\n",
        "  output = []\n",
        "  cur_count = 0\n",
        "  cur_words = []\n",
        "\n",
        "  for word in words:\n",
        "    cur_words.append(word)\n",
        "    cur_count += 1\n",
        "    if cur_count == num_words:\n",
        "      output.append(' '.join(cur_words))\n",
        "      cur_words = []\n",
        "      cur_count = 0\n",
        "  output.append(''.join(cur_words))\n",
        "  return output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLsOgY42sE0w",
        "outputId": "04f51558-3080-4dc7-ed61-b7f2a52ace15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chunking in /usr/local/lib/python3.10/dist-packages (0.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "  data = ' '.join(brown.words()[:10000])\n",
        "  num_words = 2000\n",
        "  chunks = []\n",
        "  counter = 0\n",
        "  text_chunks = splitter(data, num_words)\n",
        "  for text in text_chunks:\n",
        "    chunk = {'index': counter, 'text': text}\n",
        "    chunks.append(chunk)\n",
        "    counter += 1"
      ],
      "metadata": {
        "id": "JRkqXoTosQR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(min_df=5, max_df=.95)\n",
        "doc_term_matrix =vectorizer.fit_transform([chunk['text']for chunk in chunks])\n",
        "vocab = np.array(vectorizer.get_feature_names_out())\n",
        "print (\"\\nVocabulary:\")\n",
        "print (vocab)\n",
        "print (\"\\nDocument term matrix:\")\n",
        "chunk_names = ['Chunk-0', 'Chunk-1', 'Chunk-2', 'Chunk-3','Chunk-4']\n",
        "formatted_row = '{:>12}' * (len(chunk_names) + 1)\n",
        "print ('\\n', formatted_row.format('Word', *chunk_names), '\\n')\n",
        "for word, item in zip(vocab, doc_term_matrix.T):\n",
        "  output = [str(x) for x in item.data]\n",
        "  print (formatted_row.format(word, *output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXc8xn35sId4",
        "outputId": "c42ee595-d5d0-40fe-fb33-f2a1b8d79511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vocabulary:\n",
            "['about' 'after' 'against' 'aid' 'all' 'also' 'an' 'and' 'are' 'as' 'at'\n",
            " 'be' 'been' 'before' 'but' 'by' 'committee' 'congress' 'did' 'each'\n",
            " 'education' 'first' 'for' 'from' 'general' 'had' 'has' 'have' 'he'\n",
            " 'health' 'his' 'house' 'in' 'increase' 'is' 'it' 'last' 'made' 'make'\n",
            " 'may' 'more' 'no' 'not' 'of' 'on' 'one' 'only' 'or' 'other' 'out' 'over'\n",
            " 'pay' 'program' 'proposed' 'said' 'similar' 'state' 'such' 'take' 'than'\n",
            " 'that' 'the' 'them' 'there' 'they' 'this' 'time' 'to' 'two' 'under' 'up'\n",
            " 'was' 'were' 'what' 'which' 'who' 'will' 'with' 'would' 'year' 'years']\n",
            "\n",
            "Document term matrix:\n",
            "\n",
            "         Word     Chunk-0     Chunk-1     Chunk-2     Chunk-3     Chunk-4 \n",
            "\n",
            "       about           1           1           1           1           3\n",
            "       after           2           3           2           1           3\n",
            "     against           1           2           2           1           1\n",
            "         aid           1           1           1           3           5\n",
            "         all           2           2           5           2           1\n",
            "        also           3           3           3           4           3\n",
            "          an           5           7           5           7          10\n",
            "         and          34          27          36          36          41\n",
            "         are           5           3           6           3           2\n",
            "          as          13           4          14          18           4\n",
            "          at           5           7           9           3           6\n",
            "          be          20          14           7          10          18\n",
            "        been           7           1           6          15           5\n",
            "      before           2           2           1           1           2\n",
            "         but           3           3           2           9           5\n",
            "          by           8          22          15          14          12\n",
            "   committee           2          10           3           1           7\n",
            "    congress           1           1           3           3           1\n",
            "         did           2           1           1           2           2\n",
            "        each           1           1           4           3           1\n",
            "   education           3           2           3           1           1\n",
            "       first           4           1           4           6           3\n",
            "         for          22          19          24          27          20\n",
            "        from           4           5           6           5           5\n",
            "     general           2           2           2           3           6\n",
            "         had           3           2           7           2           6\n",
            "         has          10           2           5          20          11\n",
            "        have           4           4           4           7           5\n",
            "          he           4          13          12          13          29\n",
            "      health           1           1           2           6           1\n",
            "         his          10           6           9           3           7\n",
            "       house           5           7           4           4           2\n",
            "          in          38          27          37          49          45\n",
            "    increase           3           1           1           4           1\n",
            "          is          12           9          12          14           8\n",
            "          it          18          16           5           6           9\n",
            "        last           1           1           5           4           2\n",
            "        made           1           1           7           4           3\n",
            "        make           3           2           1           1           1\n",
            "         may           1           1           2           2           1\n",
            "        more           3           5           4           6           7\n",
            "          no           4           1           1           7           3\n",
            "         not           5           6           3          14           7\n",
            "          of          61          69          76          56          53\n",
            "          on          10          18          14          13          13\n",
            "         one           4           5           3           4           9\n",
            "        only           1           1           1           3           2\n",
            "          or           4           4           5           5           4\n",
            "       other           2           6           7           1           3\n",
            "         out           3           3           3           4           1\n",
            "        over           1           1           5           1           2\n",
            "         pay           2           3           5           4           1\n",
            "     program           2           1           4           4           5\n",
            "    proposed           2           2           1           1           1\n",
            "        said          20          15          11           9          21\n",
            "     similar           1           1           2           1           2\n",
            "       state          12           9           5           5           7\n",
            "        such           2           3           2           4           2\n",
            "        take           2           2           2           2           2\n",
            "        than           2           2           3           5           4\n",
            "        that          27          12          12          17          31\n",
            "         the         143         116         132         136         148\n",
            "        them           2           2           2           3           2\n",
            "       there           9           4           2           6           6\n",
            "        they           3           2           2           7           2\n",
            "        this           8           5           8           9           7\n",
            "        time           2           1           2           3          11\n",
            "          to          50          54          46          49          66\n",
            "         two           3           3           4           1           1\n",
            "       under           3           3           5           3           1\n",
            "          up           2           1           6           5           5\n",
            "         was          13          16          11           6          14\n",
            "        were           2           3           4           5           3\n",
            "        what           1           1           1           1           2\n",
            "       which          13          10           2           2           3\n",
            "         who           6           5           9           4           1\n",
            "        will          14           2           5          11           4\n",
            "        with           4           6           6           9          10\n",
            "       would           8          27          15           7          23\n",
            "        year           2           4           9          10           3\n",
            "       years           1           3           2           2           3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Classifier**\n",
        "\n",
        "The goal of text classification is to categorize text documents into different classes. This is an extremely\n",
        "important analysis technique in NLP."
      ],
      "metadata": {
        "id": "CHnSjw9LuOug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "category_map = {'misc.forsale': 'Sales', 'rec.motorcycles':'Motorcycles','rec.sport.baseball': 'Baseball', 'sci.crypt':'Cryptography',\n",
        "'sci.space': 'Space'}\n",
        "\n",
        "training_data = fetch_20newsgroups(subset='train',categories=category_map.keys(), shuffle=True,random_state=7)"
      ],
      "metadata": {
        "id": "VUutCtjguOE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_termcounts =vectorizer.fit_transform(training_data.data)\n",
        "print (\"\\nDimensions of training data:\", X_train_termcounts.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03umFOrVugdq",
        "outputId": "cfb3a9de-5925-4082-d6a7-3ce0828316ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dimensions of training data: (2968, 40605)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "input_data = [\"The curveballs of right handed pitchers tend to curve to the left\",\n",
        "\"Caesar cipher is an ancient form of encryption\",\n",
        "\"This two-wheeler is really good on slippery roads\"]\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf =tfidf_transformer.fit_transform(X_train_termcounts)\n",
        "\n",
        "classifier = MultinomialNB().fit(X_train_tfidf,training_data.target)\n",
        "X_input_termcounts = vectorizer.transform(input_data)\n",
        "X_input_tfidf = tfidf_transformer.transform(X_input_termcounts)\n",
        "predicted_categories = classifier.predict(X_input_tfidf)\n",
        "\n",
        "for sentence, category in zip(input_data, predicted_categories):\n",
        "  print ('\\nInput:', sentence, '\\nPredicted category:',category_map[training_data.target_names[category]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jp59We7Muh2h",
        "outputId": "77044ff7-da5e-4f58-bcac-4a990adf990b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: The curveballs of right handed pitchers tend to curve to the left \n",
            "Predicted category: Baseball\n",
            "\n",
            "Input: Caesar cipher is an ancient form of encryption \n",
            "Predicted category: Cryptography\n",
            "\n",
            "Input: This two-wheeler is really good on slippery roads \n",
            "Predicted category: Motorcycles\n"
          ]
        }
      ]
    }
  ]
}